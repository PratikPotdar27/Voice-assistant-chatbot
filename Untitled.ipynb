{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "filename = \"F_0101_17y2m_1.wav\"\n",
    "r = sr.Recognizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPA\n"
     ]
    }
   ],
   "source": [
    "with sr.AudioFile(filename) as source:\n",
    "    # listen for the data (load audio to memory)\n",
    "    audio_data = r.record(source)\n",
    "    # recognize (convert from speech to text)\n",
    "    text = r.recognize_google(audio_data)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:\\\\Users\\\\DELL\\\\Desktop\\\\deep learning\\\\HTML_Flask\\\\send'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "\n",
    "\n",
    "# importing libraries \n",
    "import speech_recognition as sr \n",
    "import os \n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "# create a speech recognition object\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# a function that splits the audio file into chunks\n",
    "# and applies speech recognition\n",
    "def get_large_audio_transcription(path):\n",
    "    \"\"\"\n",
    "    Splitting the large audio file into chunks\n",
    "    and apply speech recognition on each of these chunks\n",
    "    \"\"\"\n",
    "    # open the audio file using pydub\n",
    "    sound = AudioSegment.from_wav(path)  \n",
    "    # split audio sound where silence is 700 miliseconds or more and get chunks\n",
    "    chunks = split_on_silence(sound,\n",
    "        # experiment with this value for your target audio file\n",
    "        min_silence_len = 500,\n",
    "        # adjust this per requirement\n",
    "        silence_thresh = sound.dBFS-14,\n",
    "        # keep the silence for 1 second, adjustable as well\n",
    "        keep_silence=500,\n",
    "    )\n",
    "    folder_name = \"audio-chunks\"\n",
    "    # create a directory to store the audio chunks\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "    whole_text = \"\"\n",
    "    # process each chunk \n",
    "    for i, audio_chunk in enumerate(chunks, start=1):\n",
    "        # export audio chunk and save it in\n",
    "        # the `folder_name` directory.\n",
    "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
    "        audio_chunk.export(chunk_filename, format=\"wav\")\n",
    "        # recognize the chunk\n",
    "        with sr.AudioFile(chunk_filename) as source:\n",
    "            audio_listened = r.record(source)\n",
    "            # try converting it to text\n",
    "            try:\n",
    "                text = r.recognize_google(audio_listened)\n",
    "            except sr.UnknownValueError as e:\n",
    "                print(\"Error:\", str(e))\n",
    "            else:\n",
    "                text = f\"{text.capitalize()}. \"\n",
    "                print(chunk_filename, \":\", text)\n",
    "                whole_text += text\n",
    "    # return the text for all chunks detected\n",
    "    return print(whole_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr \n",
    "import os \n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F_0101_17y2m_1.wav'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound = AudioSegment.from_wav(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"F_0101_17y2m_1.wav\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"audio-chunks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worked code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the audio file path\n",
      "new.wav\n",
      "saving chunk0.wav\n",
      "Processing chunk 0\n",
      "Could not understand audio\n",
      "saving chunk1.wav\n",
      "Processing chunk 1\n",
      "saving chunk2.wav\n",
      "Processing chunk 2\n",
      "Could not understand audio\n",
      "saving chunk3.wav\n",
      "Processing chunk 3\n",
      "Could not understand audio\n",
      "saving chunk4.wav\n",
      "Processing chunk 4\n",
      "Could not understand audio\n",
      "saving chunk5.wav\n",
      "Processing chunk 5\n",
      "Could not understand audio\n",
      "saving chunk6.wav\n",
      "Processing chunk 6\n",
      "Could not understand audio\n",
      "saving chunk7.wav\n",
      "Processing chunk 7\n",
      "Could not understand audio\n",
      "saving chunk8.wav\n",
      "Processing chunk 8\n",
      "saving chunk9.wav\n",
      "Processing chunk 9\n",
      "Could not understand audio\n",
      "saving chunk10.wav\n",
      "Processing chunk 10\n",
      "Could not understand audio\n",
      "saving chunk11.wav\n",
      "Processing chunk 11\n",
      "Could not understand audio\n",
      "saving chunk12.wav\n",
      "Processing chunk 12\n",
      "Could not understand audio\n",
      "saving chunk13.wav\n",
      "Processing chunk 13\n",
      "Could not understand audio\n",
      "saving chunk14.wav\n",
      "Processing chunk 14\n",
      "Could not understand audio\n",
      "saving chunk15.wav\n",
      "Processing chunk 15\n",
      "Could not understand audio\n",
      "saving chunk16.wav\n",
      "Processing chunk 16\n",
      "Could not understand audio\n",
      "saving chunk17.wav\n",
      "Processing chunk 17\n",
      "saving chunk18.wav\n",
      "Processing chunk 18\n",
      "Could not understand audio\n",
      "saving chunk19.wav\n",
      "Processing chunk 19\n",
      "saving chunk20.wav\n",
      "Processing chunk 20\n",
      "Could not understand audio\n",
      "saving chunk21.wav\n",
      "Processing chunk 21\n",
      "Could not understand audio\n",
      "saving chunk22.wav\n",
      "Processing chunk 22\n",
      "Could not understand audio\n",
      "saving chunk23.wav\n",
      "Processing chunk 23\n",
      "saving chunk24.wav\n",
      "Processing chunk 24\n",
      "Could not understand audio\n",
      "saving chunk25.wav\n",
      "Processing chunk 25\n",
      "Could not understand audio\n",
      "saving chunk26.wav\n",
      "Processing chunk 26\n",
      "Could not understand audio\n",
      "saving chunk27.wav\n",
      "Processing chunk 27\n",
      "Could not understand audio\n",
      "saving chunk28.wav\n",
      "Processing chunk 28\n",
      "saving chunk29.wav\n",
      "Processing chunk 29\n",
      "Could not understand audio\n",
      "saving chunk30.wav\n",
      "Processing chunk 30\n",
      "saving chunk31.wav\n",
      "Processing chunk 31\n",
      "saving chunk32.wav\n",
      "Processing chunk 32\n",
      "Could not understand audio\n",
      "saving chunk33.wav\n",
      "Processing chunk 33\n",
      "Could not understand audio\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr \n",
    "  \n",
    "import os \n",
    "  \n",
    "from pydub import AudioSegment \n",
    "from pydub.silence import split_on_silence \n",
    "  \n",
    "# a function that splits the audio file into chunks \n",
    "# and applies speech recognition \n",
    "def silence_based_conversion(path = \"new.wav\"): \n",
    "  \n",
    "    # open the audio file stored in \n",
    "    # the local system as a wav file. \n",
    "    song = AudioSegment.from_wav(path) \n",
    "  \n",
    "    # open a file where we will concatenate   \n",
    "    # and store the recognized text \n",
    "    fh = open(\"recognized.txt\", \"w+\") \n",
    "          \n",
    "    # split track where silence is 0.5 seconds  \n",
    "    # or more and get chunks \n",
    "    chunks = split_on_silence(song, \n",
    "        # must be silent for at least 0.5 seconds \n",
    "        # or 500 ms. adjust this value based on user \n",
    "        # requirement. if the speaker stays silent for  \n",
    "        # longer, increase this value. else, decrease it. \n",
    "        #min_silence_len = 5, \n",
    "  \n",
    "        # consider it silent if quieter than -16 dBFS \n",
    "        # adjust this per requirement \n",
    "        silence_thresh = -47\n",
    "    ) \n",
    "  \n",
    "    # create a directory to store the audio chunks. \n",
    "    try: \n",
    "        os.mkdir('audio_chunks') \n",
    "    except(FileExistsError): \n",
    "        pass\n",
    "  \n",
    "    # move into the directory to \n",
    "    # store the audio files. \n",
    "    os.chdir('audio_chunks') \n",
    "  \n",
    "    i = 0\n",
    "    # process each chunk \n",
    "    for chunk in chunks: \n",
    "              \n",
    "        # Create 0.5 seconds silence chunk \n",
    "        chunk_silent = AudioSegment.silent(duration = 10) \n",
    "  \n",
    "        # add 0.5 sec silence to beginning and  \n",
    "        # end of audio chunk. This is done so that \n",
    "        # it doesn't seem abruptly sliced. \n",
    "        audio_chunk = chunk_silent + chunk + chunk_silent \n",
    "  \n",
    "        # export audio chunk and save it in  \n",
    "        # the current directory. \n",
    "        print(\"saving chunk{0}.wav\".format(i)) \n",
    "        # specify the bitrate to be 192 k \n",
    "        audio_chunk.export(\"./chunk{0}.wav\".format(i), bitrate ='92k', format =\"wav\") \n",
    "  \n",
    "        # the name of the newly created chunk \n",
    "        filename = 'chunk'+str(i)+'.wav'\n",
    "  \n",
    "        print(\"Processing chunk \"+str(i)) \n",
    "  \n",
    "        # get the name of the newly created chunk \n",
    "        # in the AUDIO_FILE variable for later use. \n",
    "        file = filename \n",
    "  \n",
    "        # create a speech recognition object \n",
    "        r = sr.Recognizer() \n",
    "  \n",
    "        # recognize the chunk \n",
    "        with sr.AudioFile(file) as source: \n",
    "            # remove this if it is not working \n",
    "            # correctly. \n",
    "            r.adjust_for_ambient_noise(source) \n",
    "            audio_listened = r.listen(source) \n",
    "  \n",
    "        try: \n",
    "            # try converting it to text \n",
    "            rec = r.recognize_google(audio_listened) \n",
    "            # write the output to the file. \n",
    "            fh.write(rec+\". \") \n",
    "  \n",
    "        # catch any errors. \n",
    "        except sr.UnknownValueError: \n",
    "            print(\"Could not understand audio\") \n",
    "  \n",
    "        except sr.RequestError as e: \n",
    "            print(\"Could not request results. check your internet connection\") \n",
    "  \n",
    "        i += 1\n",
    "  \n",
    "    os.chdir('..') \n",
    "  \n",
    "  \n",
    "if __name__ == '__main__': \n",
    "          \n",
    "    print('Enter the audio file path') \n",
    "  \n",
    "    path = input() \n",
    "  \n",
    "    silence_based_conversion(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_large_audio_transcription' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-cb382c05c55d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nFull text:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_large_audio_transcription\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'get_large_audio_transcription' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\nFull text:\", get_large_audio_transcription(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = AudioSegment.from_wav(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:\\\\Users\\\\DELL\\\\Desktop\\\\deep learning\\\\HTML_Flask\\\\F_0101_17y2m_1.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pydub.audio_segment.AudioSegment object at 0x0000023210324A90>\n"
     ]
    }
   ],
   "source": [
    "print(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr \n",
    "  \n",
    "import os \n",
    "  \n",
    "from pydub import AudioSegment \n",
    "from pydub.silence import split_on_silence \n",
    "  \n",
    "# a function that splits the audio file into chunks \n",
    "# and applies speech recognition \n",
    "path = \"F_0101_17y2m_1.wav\"\n",
    "  \n",
    "    # open the audio file stored in \n",
    "    # the local system as a wav file. \n",
    "song = AudioSegment.from_wav(path) \n",
    "  \n",
    "    # open a file where we will concatenate   \n",
    "    # and store the recognized text \n",
    "fh = open(\"recognized.txt\", \"w+\") \n",
    "          \n",
    "    # split track where silence is 0.5 seconds  \n",
    "    # or more and get chunks \n",
    "chunks = split_on_silence(song, \n",
    "        # must be silent for at least 0.5 seconds \n",
    "        # or 500 ms. adjust this value based on user \n",
    "        # requirement. if the speaker stays silent for  \n",
    "        # longer, increase this value. else, decrease it. \n",
    "        min_silence_len = 5, \n",
    "  \n",
    "        # consider it silent if quieter than -16 dBFS \n",
    "        # adjust this per requirement \n",
    "        silence_thresh = -1\n",
    ") \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-64-784f8c1549b9>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-64-784f8c1549b9>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    chunk_silent = AudioSegment.silent(duration = 10)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    os.mkdir('audio_chunks') \n",
    "\n",
    "  \n",
    "    # move into the directory to \n",
    "    # store the audio files. \n",
    "    os.chdir('audio_chunks') \n",
    "  \n",
    "    i = 0\n",
    "    i1=1\n",
    "    i2=2\n",
    "    # process each chunk \n",
    "chunk=Chunks\n",
    "              \n",
    "        # Create 0.5 seconds silence chunk \n",
    "        chunk_silent = AudioSegment.silent(duration = 10) \n",
    "  \n",
    "        # add 0.5 sec silence to beginning and  \n",
    "        # end of audio chunk. This is done so that \n",
    "        # it doesn't seem abruptly sliced. \n",
    "        audio_chunk = chunk_silent + chunk + chunk_silent \n",
    "  \n",
    "        # export audio chunk and save it in  \n",
    "        # the current directory. \n",
    "        print(\"saving chunk{0}.wav\".format(i)) \n",
    "        # specify the bitrate to be 192 k \n",
    "        audio_chunk.export(\"./chunk{0}.wav\".format(i), bitrate ='192k', format =\"wav\") \n",
    "  \n",
    "        # the name of the newly created chunk \n",
    "        filename = 'chunk'+str(i)+'.wav'\n",
    "  \n",
    "        print(\"Processing chunk \"+str(i)) \n",
    "  \n",
    "        # get the name of the newly created chunk \n",
    "        # in the AUDIO_FILE variable for later use. \n",
    "        file = filename \n",
    "  \n",
    "        # create a speech recognition object \n",
    "        r = sr.Recognizer() \n",
    "  \n",
    "        # recognize the chunk \n",
    "        with sr.AudioFile(file) as source: \n",
    "            # remove this if it is not working \n",
    "            # correctly. \n",
    "            r.adjust_for_ambient_noise(source) \n",
    "            audio_listened = r.listen(source) \n",
    "  \n",
    "        try: \n",
    "            # try converting it to text \n",
    "            rec = r.recognize_google(audio_listened) \n",
    "            # write the output to the file. \n",
    "            fh.write(rec+\". \") \n",
    "  \n",
    "        # catch any errors. \n",
    "        except sr.UnknownValueError: \n",
    "            print(\"Could not understand audio\") \n",
    "  \n",
    "        except sr.RequestError as e: \n",
    "            print(\"Could not request results. check your internet connection\") \n",
    "  \n",
    "        i += 1\n",
    "  \n",
    "    os.chdir('..') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-d2e9d82ccf87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m# end of audio chunk. This is done so that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# it doesn't seem abruptly sliced.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0maudio_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk_silent\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mchunk_silent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# export audio chunk and save it in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\pydub\\audio_segment.py\u001b[0m in \u001b[0;36m__add__\u001b[1;34m(self, arg)\u001b[0m\n\u001b[0;32m    364\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrossfade\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__radd__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrarg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\pydub\\audio_segment.py\u001b[0m in \u001b[0;36mapply_gain\u001b[1;34m(self, volume_change)\u001b[0m\n\u001b[0;32m   1111\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_gain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvolume_change\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         return self._spawn(data=audioop.mul(self._data, self.sample_width,\n\u001b[1;32m-> 1113\u001b[1;33m                                             db_to_float(float(volume_change))))\n\u001b[0m\u001b[0;32m   1114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moverlay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgain_during_overlay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "chunk=chunks\n",
    "              \n",
    "        # Create 0.5 seconds silence chunk \n",
    "chunk_silent = AudioSegment.silent(duration = 10) \n",
    "  \n",
    "        # add 0.5 sec silence to beginning and  \n",
    "        # end of audio chunk. This is done so that \n",
    "        # it doesn't seem abruptly sliced. \n",
    "audio_chunk = chunk_silent + chunk + chunk_silent \n",
    "  \n",
    "        # export audio chunk and save it in  \n",
    "        # the current directory. \n",
    "print(\"saving chunk{0}.wav\".format(i)) \n",
    "        # specify the bitrate to be 192 k \n",
    "audio_chunk.export(\"./chunk{0}.wav\".format(i), bitrate ='192k', format =\"wav\") \n",
    "  \n",
    "        # the name of the newly created chunk \n",
    "filename = 'chunk'+str(i)+'.wav'\n",
    "  \n",
    "print(\"Processing chunk \"+str(i)) \n",
    "  \n",
    "        # get the name of the newly created chunk \n",
    "        # in the AUDIO_FILE variable for later use. \n",
    "file = filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flaskwebgui'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-86e6aeb8db91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mflask\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFlask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mflaskwebgui\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFlaskUI\u001b[0m \u001b[1;31m#get the FlaskUI class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mapp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFlask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'flaskwebgui'"
     ]
    }
   ],
   "source": [
    "\n",
    "#filename = \"16-122828-0002.wav\"\n",
    "\n",
    "from flask import Flask\n",
    "from flaskwebgui import FlaskUI #get the FlaskUI class\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Feed it the flask app instance \n",
    "ui = FlaskUI(app)\n",
    "\n",
    "# do your logic as usual in Flask\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "  return \"It works!\"\n",
    "\n",
    "# call the 'run' method\n",
    "ui.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "sound = AudioSegment.from_file(\"new.wav\", format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141410"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-41.81136596657302"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sound.dBFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing voice chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "import cv2\n",
    "import PIL.Image, PIL.ImageTk\n",
    "import pyttsx3\n",
    "import datetime\n",
    "import speech_recognition as sr\n",
    "#import wikipedia\n",
    "#import webbrowser\n",
    "import os\n",
    "import random\n",
    "import smtplib\n",
    "#import roman\n",
    "#from Class1 import Student\n",
    "#import pytesseract\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "Recognizing\n",
      "0    0\n",
      "dtype: int32\n",
      "Listening...\n",
      "Recognizing\n"
     ]
    }
   ],
   "source": [
    "numbers = {'hundred':100, 'thousand':1000, 'lakh':100000}\n",
    "a = {'name':'pratik'}\n",
    "engine = pyttsx3.init('sapi5')\n",
    "voices = engine.getProperty('voices')\n",
    "engine.setProperty('voice', voices[0].id)\n",
    "\n",
    "window = Tk()\n",
    "\n",
    "global var\n",
    "global var1\n",
    "\n",
    "var = StringVar()\n",
    "var1 = StringVar()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def speak(audio):\n",
    "    engine.say(audio)\n",
    "    engine.runAndWait()\n",
    "    \n",
    "def wishme():\n",
    "    hour = int(datetime.datetime.now().hour)\n",
    "    if hour >= 0 and hour <= 12:\n",
    "        var.set(\"Good Morning Customer\") #Name - your Name\n",
    "        window.update()\n",
    "        speak(\"Good Morning Customer!\")\n",
    "    elif hour >= 12 and hour <= 18:\n",
    "        var.set(\"Good Afternoon Customer!\")\n",
    "        window.update()\n",
    "        speak(\"Good Afternoon Customer!\")\n",
    "    else:\n",
    "        var.set(\"Good Evening Customer\")\n",
    "        window.update()\n",
    "        speak(\"Good Evening Customer!\")\n",
    "    speak(\"Myself Customer Service Robo! How may I help you dear\") #BotName - Give a name to your assistant\n",
    "\n",
    "    \n",
    "def takeCommand():\n",
    "    r = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        var.set(\"Listening...\")\n",
    "        window.update()\n",
    "        print(\"Listening...\")\n",
    "        r.pause_threshold = 1\n",
    "        r.energy_threshold = 400\n",
    "        audio = r.listen(source)\n",
    "    try:\n",
    "        var.set(\"Recognizing...\")\n",
    "        window.update()\n",
    "        print(\"Recognizing\")\n",
    "        query = r.recognize_google(audio, language='en')\n",
    "    except Exception as e:\n",
    "        return \"None\"\n",
    "    var1.set(query)\n",
    "    window.update()\n",
    "    return query\n",
    "\n",
    "\n",
    "def query():\n",
    "    btn2['state'] = 'disabled'\n",
    "    btn0['state'] = 'disabled'\n",
    "    btn1.configure(bg = 'orange')\n",
    "    wishme()\n",
    "    while True:\n",
    "        btn1.configure(bg = 'orange')\n",
    "        query = takeCommand().lower()\n",
    "\n",
    "\n",
    "        if 'exit' in query:\n",
    "            var.set(\"Bye Dear Customer\")\n",
    "            btn1.configure(bg = '#5C85FB')\n",
    "            btn2['state'] = 'normal'\n",
    "            btn0['state'] = 'normal'\n",
    "            window.update()\n",
    "            speak(\"Bye Dear Customer\")\n",
    "            break \n",
    "            \n",
    "        else:\n",
    "            voc_size=5000\n",
    "            import nltk\n",
    "            import re\n",
    "            from nltk.corpus import stopwords\n",
    "            from nltk.stem.porter import PorterStemmer\n",
    "            ps = PorterStemmer()\n",
    "            corpus = []\n",
    "            review = re.sub('[^a-zA-Z]', ' ', query)\n",
    "            review = review.lower()\n",
    "            review = review.split()\n",
    "            review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "            review = ' '.join(review)\n",
    "            corpus.append(review)\n",
    "        \n",
    "            from tensorflow.keras.layers import Embedding\n",
    "            from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "            from tensorflow.keras.models import Sequential\n",
    "            from tensorflow.keras.preprocessing.text import one_hot\n",
    "            from tensorflow.keras.layers import LSTM\n",
    "            from tensorflow.keras.layers import Dense\n",
    "    \n",
    "            onehot_repr=[one_hot(words,voc_size)for words in corpus]\n",
    "    \n",
    "            sent_length=20\n",
    "            embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
    "            import numpy as np\n",
    "            X_final=np.array(embedded_docs)\n",
    "            from tensorflow.keras.models import save_model, load_model\n",
    "            filepath= './saved_model'\n",
    "            model=load_model(filepath, compile = True)\n",
    "            predctn= model.predict_classes(X_final)        \n",
    "            \n",
    "            #gg1.drop(0)\n",
    "            #g1.drop(0)\n",
    "            gg=np.reshape(predctn,1)\n",
    "            gg1=pd.Series(gg)\n",
    "            g1=gg1.get(key=0)\n",
    "        \n",
    "            print(gg1)        \n",
    "            if g1==0:\n",
    "                speak(\"Thank you for reaching out. We are sorry for your inconvenience. We have successfully reported your query\")\n",
    "            else:\n",
    "                speak(\"Thank you for your kind word. We are always ready for your service\")\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "def update(ind):\n",
    "    frame = frames[(ind)%100]\n",
    "    ind += 1\n",
    "    label.configure(image=frame)\n",
    "    window.after(100, update, ind)\n",
    "\n",
    "label2 = Label(window, textvariable = var1, bg = '#FAB60C')\n",
    "label2.config(font=(\"Courier\", 20))\n",
    "var1.set('User Said:')\n",
    "label2.pack()\n",
    "\n",
    "label1 = Label(window, textvariable = var, bg = '#ADD8E6')\n",
    "label1.config(font=(\"Courier\", 20))\n",
    "var.set('Welcome')\n",
    "label1.pack()\n",
    "\n",
    "frames = [PhotoImage(file='Assistant.gif',format = 'gif -index %i' %(i)) for i in range(100)]\n",
    "window.title('Customer Care Robo')\n",
    "\n",
    "label = Label(window, width = 500, height = 500)\n",
    "label.pack()\n",
    "window.after(0, update, 0)\n",
    "\n",
    "btn0 = Button(text = 'WISH ME',width = 20, command = wishme, bg = '#5C85FB')\n",
    "btn0.config(font=(\"Courier\", 12))\n",
    "btn0.pack()\n",
    "btn1 = Button(text = 'query',width = 20,command = query, bg = '#5C85FB')\n",
    "btn1.config(font=(\"Courier\", 12))\n",
    "btn1.pack()\n",
    "btn2 = Button(text = 'EXIT',width = 20, command = window.destroy, bg = '#5C85FB')\n",
    "btn2.config(font=(\"Courier\", 12))\n",
    "btn2.pack()\n",
    "\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
